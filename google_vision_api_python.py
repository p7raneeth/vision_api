# -*- coding: utf-8 -*-
"""google_vision_api_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18PK0lUpwVqTltRnNY-8rpGxmRk1lZagc
"""



import numpy as np
import torch
from google.cloud import vision
import os, io
import base64
import json
from google.cloud import vision
from google.cloud import vision_v1
from google.cloud.vision_v1 import types
import pandas as pd
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('distiluse-base-multilingual-cased-v1')

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'gspyapi-310804-85785668e19b.json'
vision_client = vision.ImageAnnotatorClient()
FILE_NAME = 'oreo_arabic.png'
FOLDER_PATH = r'./'


def detect_text(path):
    """Detects text in the file."""
    from google.cloud import vision
    import io
    client = vision.ImageAnnotatorClient()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision.Image(content=content)

    response = client.text_detection(image=image)
    texts = response.text_annotations
    print('Texts:')
    
    formatted_str = ''

    for text in texts:
        formatted_str += text.description
    return formatted_str, response




def extract_block_level_data(lang_code):
    extracted_text_ingredients = ""
    block_level_info = []
    for k in range(14):
        for p in range(len(response.full_text_annotation.pages[0].blocks[k].paragraphs)):
            word_len = len(response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words)
            for j in range(word_len):
                try:
                    if response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].property.detected_languages[0].language_code == lang_code:
                        symbols_len = len(response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols)
                        for i in range(symbols_len):
                            try:
                                if response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].property.detected_break.type_:
                                        extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                                        extracted_text_ingredients += ' '
                                else:
                                    extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                            except:
                                extracted_text_ingredients += ' '
                                extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                except:
                    symbols_len = len(response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols)
                    for i in range(symbols_len):
                        try:
                            if response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].property.detected_break.type_:
                                    extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                                    extracted_text_ingredients += ' '
                            else:
                                extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                        except:
                            extracted_text_ingredients += ' '
                            extracted_text_ingredients += response.full_text_annotation.pages[0].blocks[k].paragraphs[p].words[j].symbols[i].text
                            print(extracted_text_ingredients)
        if extracted_text_ingredients != '':                     
          block_level_info.append(extracted_text_ingredients)
        extracted_text = extracted_text_ingredients.replace('"', "") 
        extracted_text_ingredients = ''

    return block_level_info

if __name__ == "__main__":

  texts, response = detect_text(os.path.join(FOLDER_PATH, FILE_NAME))
  block_level_data = extract_block_level_data('ar')
  print(len(block_level_data))
  q_list = ["ingredients include"]
  q_embedding = model.encode(q_list, convert_to_tensor=True)
  block_embedding = model.encode(block_level_data, convert_to_tensor=True)
  for i in range(1):
    cosine_scores = util.cos_sim(q_embedding[i], block_embedding)
  print(cosine_scores)
  index = torch.argmax(cosine_scores)
  print('index: ', index)
  ingredients_data = block_level_data[index]
  print(ingredients_data)
  print(f'this sentence is related to {q_list[0]} :', ingredients_data.split('.')[1])
  print('this is legal designation :', ingredients_data.split('.')[0])

